{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T18:12:12.885605Z",
     "start_time": "2025-08-05T18:12:12.876225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.feature_selection import RFECV"
   ],
   "id": "38d59690e249caca",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T17:26:42.943726Z",
     "start_time": "2025-08-05T17:26:42.869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs = [pd.read_parquet('../Outputs/cleaned-5-player-features-camera-rounds.parquet'),\n",
    "       pd.read_parquet('../Outputs/cleaned-5-player-features-3d-rounds.parquet'),\n",
    "       pd.read_parquet('../Outputs/cleaned-5-player-features-camera-halves.parquet'),\n",
    "       pd.read_parquet('../Outputs/cleaned-5-player-features-3d-halves.parquet')\n",
    "       ]"
   ],
   "id": "30a849bd64b3c10e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T17:26:42.971391Z",
     "start_time": "2025-08-05T17:26:42.961065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add column for the number of seconds players are alive to rounds dfs\n",
    "for i in range(5):\n",
    "    dfs[0][f'p{i+1}_time_alive'] = (dfs[0][f'p{i+1}_num_samples_d'] + dfs[0][f'p{i+1}_num_samples_j']) / 2\n",
    "    dfs[1][f'p{i+1}_time_alive'] = (dfs[1][f'p{i+1}_num_samples_d'] + dfs[1][f'p{i+1}_num_samples_j']) / 2"
   ],
   "id": "429b9a28181cd2a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T17:26:43.084577Z",
     "start_time": "2025-08-05T17:26:42.985643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "half_metrics = [\n",
    "    'ch_area', 'ch_volume', 'ch_area_normed', 'ch_volume_normed',\n",
    "    'frac_dim',\n",
    "    'C1', 'C2', 'C3', 'C4', 'C5',\n",
    "    'alpha_d', 'alpha_j', 'num_samples_d','num_samples_j',\n",
    "]\n",
    "\n",
    "round_metrics = half_metrics + ['time_alive']\n",
    "\n",
    "# Statistic suffixes\n",
    "stats = ['min', 'max', 'mean',  'std']\n",
    "\n",
    "aggregate_dfs = []\n",
    "for i in range(len(dfs)):\n",
    "       if i <= 1:\n",
    "              metrics = round_metrics\n",
    "              # List comprehension for output column names\n",
    "              output_columns = [f\"{metric}_{stat}\" for metric in metrics for stat in stats]\n",
    "\n",
    "              # Iterate through each metric and calculate row-wise stats\n",
    "              for metric in metrics:\n",
    "                     cols = [f'p{i+1}_{metric}' for i in range(5)]\n",
    "\n",
    "                     # Convert to a 2D array for row-wise operation\n",
    "                     data = dfs[i][cols].to_numpy()\n",
    "\n",
    "                     # Add new columns with row-wise aggregations\n",
    "                     dfs[i][f'{metric}_min'] = np.min(data, axis=1)\n",
    "                     dfs[i][f'{metric}_max'] = np.max(data, axis=1)\n",
    "                     dfs[i][f'{metric}_mean'] = np.mean(data, axis=1)\n",
    "                     dfs[i][f'{metric}_std'] = np.std(data, axis=1)\n",
    "              aggregate_dfs.append(dfs[i][['matchID', 'mapName', 'side', 'roundNum', 'Label'] + output_columns])\n",
    "       else:\n",
    "              metrics = half_metrics\n",
    "              # List comprehension for output column names\n",
    "              output_columns = [f\"{metric}_{stat}\" for metric in metrics for stat in stats]\n",
    "\n",
    "              # Iterate through each metric and calculate row-wise stats\n",
    "              for metric in metrics:\n",
    "                     cols = [f'p{i+1}_{metric}' for i in range(5)]\n",
    "\n",
    "                     # Convert to a 2D array for row-wise operation\n",
    "                     data = dfs[i][cols].to_numpy()\n",
    "\n",
    "                     # Add new columns with row-wise aggregations\n",
    "                     dfs[i][f'{metric}_min'] = np.min(data, axis=1)\n",
    "                     dfs[i][f'{metric}_max'] = np.max(data, axis=1)\n",
    "                     dfs[i][f'{metric}_mean'] = np.mean(data, axis=1)\n",
    "                     dfs[i][f'{metric}_std'] = np.std(data, axis=1)\n",
    "              aggregate_dfs.append(dfs[i][['matchID', 'mapName', 'side', 'team', 'Label'] + output_columns])"
   ],
   "id": "c7487a7ffb8b5570",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T17:26:43.174005Z",
     "start_time": "2025-08-05T17:26:43.108391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop any rounds where a player was alive for less than 30 seconds in the rounds dfs\n",
    "aggregate_dfs[0] = aggregate_dfs[0].loc[aggregate_dfs[0].time_alive_min >= 30]\n",
    "aggregate_dfs[1] = aggregate_dfs[1].loc[aggregate_dfs[1].time_alive_min >= 30]\n",
    "\n",
    "# Create the merged 3D and Camera Halves df as well\n",
    "aggregate_dfs.append(pd.merge(aggregate_dfs[3], aggregate_dfs[2], on=['matchID', 'mapName', 'side', 'team', 'Label'], suffixes=('_3d', '_cam')))\n",
    "aggregate_dfs.append(pd.merge(aggregate_dfs[1], aggregate_dfs[0], on=['matchID', 'mapName', 'side', 'roundNum', 'Label'], suffixes=('_3d', '_cam')))"
   ],
   "id": "94edc51c59f3491",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T17:26:43.206083Z",
     "start_time": "2025-08-05T17:26:43.196807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make a dict\n",
    "dfs_dict  = {'Camera Rounds': aggregate_dfs[0], '3D Rounds': aggregate_dfs[1], 'Camera Halves': aggregate_dfs[2], '3D Halves': aggregate_dfs[3], '3D & Camera Halves': aggregate_dfs[4], '3D & Camera Rounds': aggregate_dfs[5]}"
   ],
   "id": "1f9d2a357a99764e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T17:42:19.083828Z",
     "start_time": "2025-08-05T17:42:19.048639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rounds are too short and too noisy to accurately model the journey / dwell distributions\n",
    "# Halves are long enough where the power law behavior emerges\n",
    "# With cameras they move much faster so it is not reasonable to detect when they stop accurately leading to non decay fits for journey often\n",
    "# 3D Halves show considerable agreement with human mobility patterns and there is enough data so this makes sense\n",
    "for name, df in list(dfs_dict.items())[:4]:\n",
    "    print(name)\n",
    "    print(df.shape)\n",
    "    print(df.loc[df.alpha_j_max > 1].shape)\n",
    "    print(df.loc[df.alpha_d_max > 1].shape)\n",
    "    print(df.loc[(df.alpha_d_max <= 1) & (df.alpha_j_max <= 1)].shape)\n"
   ],
   "id": "818f74c3377481d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Rounds\n",
      "(10875, 65)\n",
      "(9255, 65)\n",
      "(8248, 65)\n",
      "(688, 65)\n",
      "3D Rounds\n",
      "(17800, 65)\n",
      "(10456, 65)\n",
      "(10158, 65)\n",
      "(4009, 65)\n",
      "Camera Halves\n",
      "(1357, 61)\n",
      "(183, 61)\n",
      "(11, 61)\n",
      "(1172, 61)\n",
      "3D Halves\n",
      "(1359, 61)\n",
      "(2, 61)\n",
      "(1, 61)\n",
      "(1356, 61)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T18:00:23.747756Z",
     "start_time": "2025-08-05T18:00:23.733121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dictionary of feature sets for various ablation studies\n",
    "feature_sets = {\n",
    "    \"All\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std',\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std', 'frac_dim_min',\n",
    "        'frac_dim_max', 'frac_dim_mean', 'frac_dim_std', 'C1_min', 'C1_max',\n",
    "        'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std', 'C3_min',\n",
    "        'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std', 'alpha_d_min', 'alpha_d_max',\n",
    "        'alpha_d_mean', 'alpha_d_std', 'alpha_j_min', 'alpha_j_max',\n",
    "        'alpha_j_mean', 'alpha_j_std'\n",
    "    ],\n",
    "\n",
    "    \"No regular convex hull features\": [\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std', 'frac_dim_min',\n",
    "        'frac_dim_max', 'frac_dim_mean', 'frac_dim_std', 'C1_min', 'C1_max',\n",
    "        'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std', 'C3_min',\n",
    "        'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std', 'alpha_d_min', 'alpha_d_max',\n",
    "        'alpha_d_mean', 'alpha_d_std', 'alpha_j_min', 'alpha_j_max',\n",
    "        'alpha_j_mean', 'alpha_j_std'\n",
    "    ],\n",
    "    \"Only regular convex hull features\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std'\n",
    "    ],\n",
    "\n",
    "    \"No convex hull features at all\": [\n",
    "        'frac_dim_min', 'frac_dim_max', 'frac_dim_mean', 'frac_dim_std',\n",
    "        'C1_min', 'C1_max', 'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std',\n",
    "        'C3_min', 'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std', 'alpha_d_min', 'alpha_d_max',\n",
    "        'alpha_d_mean', 'alpha_d_std', 'alpha_j_min', 'alpha_j_max', 'alpha_j_mean', 'alpha_j_std'\n",
    "    ],\n",
    "    \"Only convex hull features\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std',\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std'\n",
    "    ],\n",
    "\n",
    "    \"No normalized convex hull features\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std',\n",
    "        'frac_dim_min', 'frac_dim_max', 'frac_dim_mean', 'frac_dim_std',\n",
    "        'C1_min', 'C1_max', 'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std',\n",
    "        'C3_min', 'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std', 'alpha_d_min', 'alpha_d_max',\n",
    "        'alpha_d_mean', 'alpha_d_std', 'alpha_j_min', 'alpha_j_max', 'alpha_j_mean', 'alpha_j_std'\n",
    "    ],\n",
    "    \"Only normalized convex hull features\": [\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std'\n",
    "    ],\n",
    "\n",
    "    \"No entropy constant features\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std',\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std', 'frac_dim_min',\n",
    "        'frac_dim_max', 'frac_dim_mean', 'frac_dim_std',\n",
    "        'alpha_d_min', 'alpha_d_max', 'alpha_d_mean', 'alpha_d_std',\n",
    "        'alpha_j_min', 'alpha_j_max', 'alpha_j_mean', 'alpha_j_std'\n",
    "    ],\n",
    "    \"Only entropy constant features\": [\n",
    "        'C1_min', 'C1_max', 'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std',\n",
    "        'C3_min', 'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std'\n",
    "    ],\n",
    "\n",
    "    \"No fractal dimension features\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std',\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std', 'C1_min', 'C1_max',\n",
    "        'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std', 'C3_min',\n",
    "        'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std', 'alpha_d_min', 'alpha_d_max',\n",
    "        'alpha_d_mean', 'alpha_d_std', 'alpha_j_min', 'alpha_j_max',\n",
    "        'alpha_j_mean', 'alpha_j_std'\n",
    "    ],\n",
    "    \"Only fractal dimension features\": [\n",
    "        'frac_dim_min', 'frac_dim_max', 'frac_dim_mean', 'frac_dim_std'\n",
    "    ],\n",
    "\n",
    "    \"No alpha features\": [\n",
    "        'ch_area_min', 'ch_area_max', 'ch_area_mean', 'ch_area_std',\n",
    "        'ch_volume_min', 'ch_volume_max', 'ch_volume_mean', 'ch_volume_std',\n",
    "        'ch_area_normed_min', 'ch_area_normed_max', 'ch_area_normed_mean',\n",
    "        'ch_area_normed_std', 'ch_volume_normed_min', 'ch_volume_normed_max',\n",
    "        'ch_volume_normed_mean', 'ch_volume_normed_std', 'frac_dim_min',\n",
    "        'frac_dim_max', 'frac_dim_mean', 'frac_dim_std', 'C1_min', 'C1_max',\n",
    "        'C1_mean', 'C1_std', 'C2_min', 'C2_max', 'C2_mean', 'C2_std', 'C3_min',\n",
    "        'C3_max', 'C3_mean', 'C3_std', 'C4_min', 'C4_max', 'C4_mean', 'C4_std',\n",
    "        'C5_min', 'C5_max', 'C5_mean', 'C5_std'\n",
    "    ],\n",
    "    \"Only alpha features\": [\n",
    "        'alpha_d_min', 'alpha_d_max', 'alpha_d_mean', 'alpha_d_std',\n",
    "        'alpha_j_min', 'alpha_j_max', 'alpha_j_mean', 'alpha_j_std'\n",
    "    ]\n",
    "}"
   ],
   "id": "ac890f0e23231782",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T18:41:29.497240Z",
     "start_time": "2025-08-05T18:41:29.448250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loop through selected DataFrames\n",
    "for df_name, df in list(dfs_dict.items())[3:4]:\n",
    "    all_features = feature_sets[\"All\"]\n",
    "\n",
    "    # Adjust features for 3D & Camera datasets\n",
    "    if df_name in ['3D & Camera Halves', '3D & Camera Rounds']:\n",
    "        all_features = [f + '_3d' for f in all_features] + [f + '_cam' for f in all_features]\n",
    "\n",
    "    # Filter only '_mean' features\n",
    "    all_features = [f for f in all_features if '_mean' in f]\n",
    "\n",
    "    # Loop through ablation studies (feature sets)\n",
    "    for name, feature_columns in list(feature_sets.items())[3:4]:\n",
    "        # Adjust features if needed\n",
    "        if df_name in ['3D & Camera Halves', '3D & Camera Rounds']:\n",
    "            feature_columns = [f + '_3d' for f in feature_columns] + [f + '_cam' for f in feature_columns]\n",
    "\n",
    "        feature_columns = [f for f in feature_columns if '_mean' in f]\n",
    "\n",
    "        print(f\"\\n=== {df_name} - Feature Set: {name} ===\")\n",
    "        print(f\"Feature count: {len(feature_columns)}\")\n",
    "        print(feature_columns)\n",
    "\n",
    "        # Process each side individually\n",
    "        for side in np.unique(df['side']):\n",
    "            print(f\"\\n--- Side: {side} ---\")\n",
    "\n",
    "            # Filter by side\n",
    "            df_side = df[df['side'] == side]\n",
    "\n",
    "            # Extract features and labels\n",
    "            X = df_side[feature_columns].to_numpy()\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(df_side['Label'])\n",
    "\n",
    "            # Train/test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Normalize features\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            # Clustering\n",
    "            hdb = HDBSCAN(min_cluster_size=10)\n",
    "            print(f\"Training shape: {X_train.shape}\")\n",
    "            y_pred = hdb.fit_predict(X_train)\n",
    "            print(f\"Cluster labels: {np.unique(y_pred)}\")\n",
    "\n",
    "            for c in np.unique(y_pred):\n",
    "                cluster_indices = np.where(y_pred == c)[0]\n",
    "                print(f\"Cluster {c}: {cluster_indices.shape[0]} samples\")\n",
    "\n",
    "                # Cluster label breakdown\n",
    "                cluster_labels = y_train[cluster_indices]\n",
    "                unique_elements, counts = np.unique(cluster_labels, return_counts=True)\n",
    "                for i in range(len(unique_elements)):\n",
    "                    label_name = le.inverse_transform([unique_elements[i]])[0]\n",
    "                    proportion = counts[i] / np.sum(counts)\n",
    "                    print(f\"  {label_name}: {proportion:.2f}\")\n",
    "\n",
    "        # No side splitting\n",
    "        print(\"\\nNO SIDE SPLITTING VERSION\")\n",
    "        df_side = df\n",
    "        X = df_side[feature_columns].to_numpy()\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(df_side['Label'])\n",
    "\n",
    "        # Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Clustering\n",
    "        hdb = HDBSCAN(min_cluster_size=10)\n",
    "        print(f\"Training shape: {X_train.shape}\")\n",
    "        y_pred = hdb.fit_predict(X_train)\n",
    "        print(f\"Cluster labels: {np.unique(y_pred)}\")\n",
    "\n",
    "        for c in np.unique(y_pred):\n",
    "            cluster_indices = np.where(y_pred == c)[0]\n",
    "            print(f\"Cluster {c}: {cluster_indices.shape[0]} samples\")\n",
    "\n",
    "            # Cluster label breakdown\n",
    "            cluster_labels = y_train[cluster_indices]\n",
    "            unique_elements, counts = np.unique(cluster_labels, return_counts=True)\n",
    "            for i in range(len(unique_elements)):\n",
    "                label_name = le.inverse_transform([unique_elements[i]])[0]\n",
    "                proportion = counts[i] / np.sum(counts)\n",
    "                print(f\"  {label_name}: {proportion:.2f}\")"
   ],
   "id": "85e90f77039cca40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3D Halves - Feature Set: No convex hull features at all ===\n",
      "Feature count: 8\n",
      "['frac_dim_mean', 'C1_mean', 'C2_mean', 'C3_mean', 'C4_mean', 'C5_mean', 'alpha_d_mean', 'alpha_j_mean']\n",
      "\n",
      "--- Side: CT ---\n",
      "Training shape: (543, 8)\n",
      "Cluster labels: [-1]\n",
      "Cluster -1: 543 samples\n",
      "  de_ancient_CT: 0.13\n",
      "  de_dust2_CT: 0.11\n",
      "  de_inferno_CT: 0.19\n",
      "  de_mirage_CT: 0.19\n",
      "  de_nuke_CT: 0.16\n",
      "  de_overpass_CT: 0.10\n",
      "  de_vertigo_CT: 0.12\n",
      "\n",
      "--- Side: T ---\n",
      "Training shape: (544, 8)\n",
      "Cluster labels: [-1]\n",
      "Cluster -1: 544 samples\n",
      "  de_ancient_T: 0.11\n",
      "  de_dust2_T: 0.10\n",
      "  de_inferno_T: 0.20\n",
      "  de_mirage_T: 0.20\n",
      "  de_nuke_T: 0.17\n",
      "  de_overpass_T: 0.11\n",
      "  de_vertigo_T: 0.11\n",
      "\n",
      "NO SIDE SPLITTING VERSION\n",
      "Training shape: (1087, 8)\n",
      "Cluster labels: [-1]\n",
      "Cluster -1: 1087 samples\n",
      "  de_ancient_CT: 0.06\n",
      "  de_ancient_T: 0.06\n",
      "  de_dust2_CT: 0.06\n",
      "  de_dust2_T: 0.05\n",
      "  de_inferno_CT: 0.09\n",
      "  de_inferno_T: 0.10\n",
      "  de_mirage_CT: 0.09\n",
      "  de_mirage_T: 0.10\n",
      "  de_nuke_CT: 0.09\n",
      "  de_nuke_T: 0.08\n",
      "  de_overpass_CT: 0.05\n",
      "  de_overpass_T: 0.05\n",
      "  de_vertigo_CT: 0.06\n",
      "  de_vertigo_T: 0.05\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73c3900a40bb4771"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
